# Clarogent AI: Enterprise Batch Document Intelligence

![Clarogent AI](https://img.shields.io/badge/Clarogent-Enterprise%20v2.7-0052CC?style=for-the-badge&logo=openai&logoColor=white)
![Python 3.12](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Groq LPU](https://img.shields.io/badge/Inference-Groq%20LPU-orange?style=for-the-badge)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

**Clarogent AI** is a high-throughput, enterprise-grade document intelligence platform designed to automate large-scale field audits. It leverages state-of-the-art specialized Vision Language Models (Llama 3.2 Vision) running on Groq LPUs to extract critical data from complex, unstructured images (Solar Module Serials, Inverter IDs) and detect compliance violations (Human Presence) in real-time.

---

## ðŸš€ Key Features

### ðŸ›¡ï¸ **Robust Vision Pipeline**

- **Dual-Pass Vision Engine:**
  - **Pass 1 (Fast Path):** Analyzes raw, high-fidelity images directly using Llama 3.2 for maximum OCR accuracy on modern sensors.
  - **Pass 2 (Forensic Fallback):** Automatically engages computer vision enhancement algorithms (Adaptive Thresholding, Contrast Boosting, Sharpening) if the initial pass yields low-confidence results or fails validation.
- **Strict Validation Rules:** Domain-specific logic enforces correct formats (e.g., specific 16-character alphanumeric patterns for Solar Modules), rejecting hallucinations.

### âš¡ **High-Performance Architecture**

- **Parallel Swarm Processing:** Utilizes a custom `ThreadPoolExecutor` backend to process **5 concurrent rows**, achieving meaningful throughput gains over sequential processing while adhering to strict API rate limits.
- **Smart Bandwidth Optimization:** Automatically resizes and compresses high-resolution assets (down to 1280px / 85% quality) _before_ transmission, reducing network payload by **~70%**.
- **Resilient Networking:** Implements exponential backoff and independent connection pooling to handle network jitters and API throttling gracefully.

### ðŸ”’ **Enterprise Reliability**

- **Crash-Proof Execution:** `try-finally` guarded resource management ensures zero temporary file leakage, even during catastrophic failures.
- **Thread-Safe I/O:** Implements mutex-locked write operations preventing data corruption when multiple workers update the audit report simultaneously.

---

## ðŸ› ï¸ Installation & Setup

### **Prerequisites**

- **Python 3.10+** (Python 3.12 Recommended)
- A valid **Groq API Key** ([Get one here](https://console.groq.com/))
- **Internet Connection** capable of parallel downloads from Google Drive.

### **1. Clone Repository**

```bash
git clone https://github.com/chinmayaj88/Clarogent-AI.git
cd Clarogent-AI
```

### **2. Setup Environment**

It is recommended to use a virtual environment.

```bash
# Create Virtual Environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate
```

### **3. Install Dependencies**

```bash
pip install -r requirements.txt
```

### **4. Configuration (.env)**

Create a `.env` file in the root directory:

```bash
GROQ_API_KEY=gsk_your_actual_api_key_here
MAX_ROWS_LIMIT=0  # Set to 0 to process all rows, or 10 for dev testing
```

---

## ðŸ–¥ï¸ Usage Guide

### **Running the Application**

Launch the customized Streamlit interface:

```bash
streamlit run app.py
```

### **Workflow**

1.  **Ingestion:** Upload an Excel file containing columns with **Google Drive Links** (e.g., "Module Photo").
2.  **Auto-Detection:** The system automatically detects if the sheet is a "Solar Audit" or a "Generic Document" batch based on header analysis.
3.  **Processing:** Click **Start Processing**. The UI locks to prevent accidental interference.
4.  **Monitoring:** Watch real-time logs in the "Pulse" console as the Swarm captures and analyzes data.
5.  **Export:** Download the fully enriched Excel report with new columns:
    - `AI Module Serial Number`
    - `Human Detected (Yes/No)`
    - `AI Remarks` (containing validation errors or forensic notes)

---

## âš¡ Performance Optimization Report

We have completely re-engineered the processing pipeline to mitigate the bottleneck of **"Data Gravity"** (the latency of moving images from Google Drive to the local inference engine).

### **Benchmarks**

| Metric               | Legacy (Sequential) | **Clarogent V2 (Swarm)**    | Improvement         |
| :------------------- | :------------------ | :-------------------------- | :------------------ |
| **Concurrency**      | 1 Row               | **5 Concurrent Rows**       | **5x**              |
| **Network Strategy** | Shared Session      | **Independent Connections** | **Latency Reduced** |
| **Payload Size**     | Original (5MB+)     | **Smart Resize (300KB)**    | **15x Smaller**     |
| **Est. Speed**       | ~15s / Row          | **~3-4s / Row**             | **~400% Faster**    |

### **Strategic Recommendation: Edge Compression**

Implementing **client-side compression** in the Field App (before upload) would compound these gains.

- **Current:** Auditors upload 5MB raw photos -> We download 5MB -> We resize to 300KB.
- **Optimized:** Field App resizes to 1024px (300KB) -> We download 300KB (Instant).
- **Benefit:** Reduces **Download Time** by ~90% without changing any server infrastructure. |

---

## ðŸ”® Future Architecture Roadmap (The "S3 Leap")

The current theoretical limit of the system is dictated by the **Download Step**. Google Drive links are webpages, not direct files, forcing a "Download â†’ Process" workflow.

**Proposed Optimization: Migration to Amazon S3 / Blob Storage**

| Feature           | **Google Drive (Current)**     | **Amazon S3 / Direct Blob** |
| :---------------- | :----------------------------- | :-------------------------- |
| **Workflow**      | `Google -> Application -> AI`  | `S3 -> AI (Direct)`         |
| **Download Step** | **Required (3-5s bottleneck)** | **SKIPPED (0s)**            |
| **AI Ingestion**  | Upload Base64 (Slow)           | **Pass URL (Instant)**      |
| **Total Latency** | **~5-8s / Row**                | **~1.5s / Row**             |

**Recommendation:** Migrating the image capturing workflow to upload directly to S3/Azure Blob would unlock near-instantaneous processing speeds.

---

## ðŸ“„ License & Contact

**License:** Self  
**Maintainer:** Chinmaya Jena  
**Version:** 2.7.0
